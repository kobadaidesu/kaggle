{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f65598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Dataframe operations\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import svm #support vector Machine\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1428863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "\n",
    "data_df = pd.concat([train_df, test_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0efe309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_372/3663789664.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\n",
      "/tmp/ipykernel_372/3663789664.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\n",
      "/tmp/ipykernel_372/3663789664.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\n",
      "/tmp/ipykernel_372/3663789664.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\n",
      "/tmp/ipykernel_372/3663789664.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\n",
      "/tmp/ipykernel_372/3663789664.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\n"
     ]
    }
   ],
   "source": [
    "data_df['Title'] = data_df['Name']\n",
    "# Cleaning name and extracting Title\n",
    "for name_string in data_df['Name']:\n",
    "    data_df['Title'] = data_df['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n",
    "\n",
    "# Replacing rare titles with more common ones\n",
    "mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n",
    "          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n",
    "data_df.replace({'Title': mapping}, inplace=True)\n",
    "titles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\n",
    "for title in titles:\n",
    "    age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\n",
    "    data_df.loc[(data_df['Age'].isnull()) & (data_df['Title'] == title), 'Age'] = age_to_impute\n",
    "    \n",
    "# Substituting Age values in TRAIN_DF and TEST_DF:\n",
    "train_df['Age'] = data_df['Age'][:891]\n",
    "test_df['Age'] = data_df['Age'][891:]\n",
    "\n",
    "# Dropping Title feature\n",
    "data_df.drop('Title', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307b2d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Family_Size'] = data_df['Parch'] + data_df['SibSp']\n",
    "\n",
    "# Substituting Age values in TRAIN_DF and TEST_DF:\n",
    "train_df['Family_Size'] = data_df['Family_Size'][:891]\n",
    "test_df['Family_Size'] = data_df['Family_Size'][891:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f1a714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_372/3270763138.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_df['Fare'].fillna(data_df['Fare'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passengers with family survival information: 420\n"
     ]
    }
   ],
   "source": [
    "data_df['Last_Name'] = data_df['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
    "data_df['Fare'].fillna(data_df['Fare'].mean(), inplace=True)\n",
    "\n",
    "DEFAULT_SURVIVAL_VALUE = 0.5\n",
    "data_df['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n",
    "\n",
    "for grp, grp_df in data_df[['Perished','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n",
    "                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n",
    "    \n",
    "    if (len(grp_df) != 1):\n",
    "        # A Family group is found.\n",
    "        for ind, row in grp_df.iterrows():\n",
    "            smax = grp_df.drop(ind)['Perished'].max()\n",
    "            smin = grp_df.drop(ind)['Perished'].min()\n",
    "            passID = row['PassengerId']\n",
    "            if (smax == 1.0):\n",
    "                data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "            elif (smin==0.0):\n",
    "                data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "\n",
    "print(\"Number of passengers with family survival information:\", \n",
    "      data_df.loc[data_df['Family_Survival']!=0.5].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf3592e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passenger with family/group survival information: 546\n"
     ]
    }
   ],
   "source": [
    "for _, grp_df in data_df.groupby('Ticket'):\n",
    "    if (len(grp_df) != 1):\n",
    "        for ind, row in grp_df.iterrows():\n",
    "            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n",
    "                smax = grp_df.drop(ind)['Perished'].max()\n",
    "                smin = grp_df.drop(ind)['Perished'].min()\n",
    "                passID = row['PassengerId']\n",
    "                if (smax == 1.0):\n",
    "                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "                elif (smin==0.0):\n",
    "                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "                        \n",
    "print(\"Number of passenger with family/group survival information: \" \n",
    "      +str(data_df[data_df['Family_Survival']!=0.5].shape[0]))\n",
    "\n",
    "# # Family_Survival in TRAIN_DF and TEST_DF:\n",
    "train_df['Family_Survival'] = data_df['Family_Survival'][:891]\n",
    "test_df['Family_Survival'] = data_df['Family_Survival'][891:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eefd5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_372/1744406990.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_df['Fare'].fillna(data_df['Fare'].median(), inplace = True)\n"
     ]
    }
   ],
   "source": [
    "data_df['Fare'].fillna(data_df['Fare'].median(), inplace = True)\n",
    "\n",
    "# Making Bins\n",
    "data_df['FareBin'] = pd.qcut(data_df['Fare'], 5)\n",
    "\n",
    "label = LabelEncoder()\n",
    "data_df['FareBin_Code'] = label.fit_transform(data_df['FareBin'])\n",
    "\n",
    "train_df['FareBin_Code'] = data_df['FareBin_Code'][:891]\n",
    "test_df['FareBin_Code'] = data_df['FareBin_Code'][891:]\n",
    "\n",
    "train_df.drop(['Fare'], axis=1, inplace=True)\n",
    "test_df.drop(['Fare'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1575edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['AgeBin'] = pd.qcut(data_df['Age'], 4)\n",
    "\n",
    "label = LabelEncoder()\n",
    "data_df['AgeBin_Code'] = label.fit_transform(data_df['AgeBin'])\n",
    "\n",
    "train_df['AgeBin_Code'] = data_df['AgeBin_Code'][:891]\n",
    "test_df['AgeBin_Code'] = data_df['AgeBin_Code'][891:]\n",
    "\n",
    "train_df.drop(['Age'], axis=1, inplace=True)\n",
    "test_df.drop(['Age'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a7f1f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\n",
    "test_df['Sex']  = test_df['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# PassengerId を保存しておく\n",
    "test_passenger_ids = test_df['PassengerId'].copy()\n",
    "\n",
    "train_df.drop(['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\n",
    "               'Embarked'], axis = 1, inplace = True)\n",
    "test_df.drop(['Name','PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\n",
    "              'Embarked'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2eab420",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop('Perished', axis = 1)\n",
    "y = train_df['Perished']\n",
    "X_test = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7251beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X = std_scaler.fit_transform(X)\n",
    "X_test = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdfdc942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 240 candidates, totalling 2400 fits\n",
      "Best CV Score: 0.8661474407944996\n",
      "Best Parameters: KNeighborsClassifier(leaf_size=11, n_neighbors=14)\n",
      "\n",
      "================================================================================\n",
      "Fold ごとの Train/Validation スコアと Gap\n",
      "================================================================================\n",
      "Fold  1 | Train: 0.900215 | Val: 0.856209 | Gap: 0.044006\n",
      "Fold  2 | Train: 0.903704 | Val: 0.778986 | Gap: 0.124719\n",
      "Fold  3 | Train: 0.903846 | Val: 0.775134 | Gap: 0.128712\n",
      "Fold  4 | Train: 0.899587 | Val: 0.876010 | Gap: 0.023577\n",
      "Fold  5 | Train: 0.897427 | Val: 0.864865 | Gap: 0.032562\n",
      "Fold  6 | Train: 0.893729 | Val: 0.911732 | Gap: -0.018004\n",
      "Fold  7 | Train: 0.902812 | Val: 0.821282 | Gap: 0.081530\n",
      "Fold  8 | Train: 0.894989 | Val: 0.860931 | Gap: 0.034059\n",
      "Fold  9 | Train: 0.894129 | Val: 0.917514 | Gap: -0.023385\n",
      "Fold 10 | Train: 0.892593 | Val: 0.915855 | Gap: -0.023262\n",
      "================================================================================\n",
      "平均 Train スコア: 0.898303 (±0.004102)\n",
      "平均 Val スコア:   0.857852 (±0.049616)\n",
      "平均 Gap:          0.040451 (±0.053414)\n",
      "OOF スコア:        0.858485\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "n_neighbors = [6,7,8,9,10,11,12,14,16,18,20,22]\n",
    "algorithm = ['auto']\n",
    "weights = ['uniform', 'distance']\n",
    "leaf_size = list(range(1,50,5))\n",
    "hyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, \n",
    "               'n_neighbors': n_neighbors}\n",
    "\n",
    "# GridSearchCV で最適なハイパーパラメータを探索\n",
    "gd = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, verbose=True, \n",
    "                  cv=10, scoring = \"roc_auc\")\n",
    "gd.fit(X, y)\n",
    "print(f\"Best CV Score: {gd.best_score_}\")\n",
    "print(f\"Best Parameters: {gd.best_estimator_}\")\n",
    "\n",
    "# Fold ごとの詳細スコアを表示\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Fold ごとの Train/Validation スコアと Gap\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# KFold で再実行して詳細を表示\n",
    "cv = KFold(n_splits=10, shuffle=False)\n",
    "best_model = gd.best_estimator_\n",
    "\n",
    "oof_predictions = np.zeros(len(X))\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "gaps = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # モデルを訓練\n",
    "    model = KNeighborsClassifier(**gd.best_params_)\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Train スコア\n",
    "    train_pred_proba = model.predict_proba(X_train_fold)[:, 1]\n",
    "    train_score = roc_auc_score(y_train_fold, train_pred_proba)\n",
    "    \n",
    "    # Validation スコア\n",
    "    val_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "    val_score = roc_auc_score(y_val_fold, val_pred_proba)\n",
    "    \n",
    "    # Gap\n",
    "    gap = train_score - val_score\n",
    "    \n",
    "    # OOF predictions を保存\n",
    "    oof_predictions[val_idx] = val_pred_proba\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "    gaps.append(gap)\n",
    "    \n",
    "    print(f\"Fold {fold:2d} | Train: {train_score:.6f} | Val: {val_score:.6f} | Gap: {gap:.6f}\")\n",
    "\n",
    "# OOF スコア\n",
    "oof_score = roc_auc_score(y, oof_predictions)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"平均 Train スコア: {np.mean(train_scores):.6f} (±{np.std(train_scores):.6f})\")\n",
    "print(f\"平均 Val スコア:   {np.mean(val_scores):.6f} (±{np.std(val_scores):.6f})\")\n",
    "print(f\"平均 Gap:          {np.mean(gaps):.6f} (±{np.std(gaps):.6f})\")\n",
    "print(f\"OOF スコア:        {oof_score:.6f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae866a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy配列です。NaN数: 0\n"
     ]
    }
   ],
   "source": [
    "if isinstance(X_test, np.ndarray):\n",
    "    print(\"NumPy配列です。NaN数:\", np.isnan(X).sum())\n",
    "else:\n",
    "    print(\"DataFrameです。NaN数:\")\n",
    "    print(X_test.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17bba46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_imputed shape: (418, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# strategy を指定して、すべての列を保持する\n",
    "imputer = SimpleImputer(strategy='median')  # または 'mean', 'most_frequent'\n",
    "\n",
    "# 重要：訓練データでfit、テストデータでtransformを分ける\n",
    "imputer.fit(X)  # X で学習\n",
    "X_test_imputed = imputer.transform(X_test)  # X_test に適用\n",
    "\n",
    "print(\"X_test_imputed shape:\", X_test_imputed.shape)  # (418, 6) になるはず\n",
    "\n",
    "y_pred = gd.best_estimator_.predict(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aac618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (891, 6)\n",
      "X_test shape: (418, 6)\n",
      "X_test_imputed shape: (418, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"X_test_imputed shape:\", X_test_imputed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54baf7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputer を作成して X と X_test を補完\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# モデルを訓練・予測\n",
    "knn = KNeighborsClassifier(leaf_size=11,\n",
    "                           n_neighbors=14,\n",
    "                           weights='uniform')\n",
    "knn.fit(X_imputed, y)\n",
    "y_pred = knn.predict(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "227eec72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: ../output/konstantin.csv\n",
      "   PassengerId  Perished\n",
      "0          892         1\n",
      "1          893         0\n",
      "2          894         1\n",
      "3          895         1\n",
      "4          896         0\n"
     ]
    }
   ],
   "source": [
    "# Submission ファイルを作成\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_passenger_ids,\n",
    "    'Perished': y_pred\n",
    "})\n",
    "\n",
    "\n",
    "submission.to_csv(\"../output/konstantin.csv\", index=False)\n",
    "\n",
    "print(\"Submission file created: ../output/konstantin.csv\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

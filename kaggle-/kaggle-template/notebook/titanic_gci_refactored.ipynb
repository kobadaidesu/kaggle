{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction - Refactored\n",
    "\n",
    "このノートブックは、Titanicデータセットを使用して生存予測を行います。\n",
    "\n",
    "## 目次\n",
    "1. セットアップとデータ読み込み\n",
    "2. データ探索\n",
    "3. データ前処理\n",
    "4. モデルトレーニング\n",
    "5. ハイパーパラメータチューニング\n",
    "6. アンサンブルモデル\n",
    "7. 予測と提出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. セットアップとデータ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ライブラリのインポート完了\n"
     ]
    }
   ],
   "source": [
    "# ライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "import string \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 定数設定\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.3\n",
    "N_JOBS = -1\n",
    "\n",
    "print(\"ライブラリのインポート完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (891, 12)\n",
      "Test shape: (418, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Perished</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Perished  Pclass  \\\n",
       "0            1         1       3   \n",
       "1            2         0       1   \n",
       "2            3         0       3   \n",
       "3            4         0       1   \n",
       "4            5         1       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データ読み込み\n",
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## 2. データ探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Train Data Info\n",
      "============================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Perished     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "\n",
      "============================================================\n",
      "Missing Values - Train\n",
      "============================================================\n",
      "PassengerId      0\n",
      "Perished         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "\n",
      "============================================================\n",
      "Missing Values - Test\n",
      "============================================================\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "\n",
      "Survival rate: 38.38%\n",
      "Death rate: 61.62%\n"
     ]
    }
   ],
   "source": [
    "# データの基本情報\n",
    "print(\"=\" * 60)\n",
    "print(\"Train Data Info\")\n",
    "print(\"=\" * 60)\n",
    "df_train.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Missing Values - Train\")\n",
    "print(\"=\" * 60)\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Missing Values - Test\")\n",
    "print(\"=\" * 60)\n",
    "print(df_test.isnull().sum())\n",
    "\n",
    "# 生存率\n",
    "survival_rate = (df_train['Perished'] == 0).sum() / len(df_train) * 100\n",
    "print(f\"\\nSurvival rate: {survival_rate:.2f}%\")\n",
    "print(f\"Death rate: {100 - survival_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "## 3. データ前処理\n",
    "\n",
    "前処理を関数化して再利用性を向上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d957a887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median age of Pclass 1 females: 35.0\n",
      "Median age of Pclass 1 females (test): 41.0\n",
      "Median age of Pclass 1 males: 40.0\n",
      "Median age of Pclass 1 males (test): 42.0\n",
      "Median age of Pclass 2 females: 28.0\n",
      "Median age of Pclass 2 females (test): 24.0\n",
      "Median age of Pclass 2 males: 30.0\n",
      "Median age of Pclass 2 males (test): 28.0\n",
      "Median age of Pclass 3 females: 21.5\n",
      "Median age of Pclass 3 females (test): 22.0\n",
      "Median age of Pclass 3 males: 25.0\n",
      "Median age of Pclass 3 males (test): 24.0\n",
      "Median age of all passengers: 28.0\n",
      "Median age of all passengers (test): 27.0\n"
     ]
    }
   ],
   "source": [
    "# 数値化を保証\n",
    "df_train['Age'] = pd.to_numeric(df_train['Age'], errors='coerce')\n",
    "df_test['Age']  = pd.to_numeric(df_test['Age'],  errors='coerce')\n",
    "\n",
    "# ここが重要：['Age'] を先に選んでから median\n",
    "age_by_pclass_sex_train = df_train.groupby(['Sex', 'Pclass'])['Age'].median()\n",
    "age_by_pclass_sex_test  = df_test.groupby(['Sex', 'Pclass'])['Age'].median()\n",
    "\n",
    "for pclass in (1, 2, 3):\n",
    "    for sex in ('female', 'male'):\n",
    "        print(f\"Median age of Pclass {pclass} {sex}s: {age_by_pclass_sex_train.loc[(sex, pclass)]}\")\n",
    "        print(f\"Median age of Pclass {pclass} {sex}s (test): {age_by_pclass_sex_test.loc[(sex, pclass)]}\")\n",
    "\n",
    "print(f\"Median age of all passengers: {df_train['Age'].median()}\")\n",
    "print(f\"Median age of all passengers (test): {df_test['Age'].median()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb205038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Age'] = pd.qcut(df_test['Age'], duplicates='drop', q=10)\n",
    "df_train['Age'] = pd.qcut(df_train['Age'], duplicates='drop', q=10)\n",
    "\n",
    "df_test['Fare'] = pd.qcut(df_test['Fare'], duplicates='drop', q=13)\n",
    "df_train['Fare'] = pd.qcut(df_train['Fare'], duplicates='drop', q=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ef9b197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deck\n",
       "A         NaN\n",
       "ABC     181.0\n",
       "DE       87.0\n",
       "FG       26.0\n",
       "M      1014.0\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\n",
    "df_train['Deck'] = df_train['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n",
    "df_test['Deck']  = df_test['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n",
    "\n",
    "# Train data processing\n",
    "df_train_decks = df_train.groupby(['Deck', 'Pclass']).count().drop(\n",
    "    columns=['Perished', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']\n",
    ").rename(columns={'Name': 'Count'}).transpose()\n",
    "\n",
    "# Test data processing - exclude 'Perished' from drop list\n",
    "df_test_decks = df_test.groupby(['Deck', 'Pclass']).count().drop(\n",
    "    columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']\n",
    ").rename(columns={'Name': 'Count'}).transpose()\n",
    "\n",
    "def get_pclass_dist(df):\n",
    "    \n",
    "    # Creating a dictionary for every passenger class count in every deck\n",
    "    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n",
    "    decks = df.columns.levels[0]    \n",
    "    \n",
    "    for deck in decks:\n",
    "        for pclass in range(1, 4):\n",
    "            try:\n",
    "                count = df[deck][pclass][0]\n",
    "                deck_counts[deck][pclass] = count \n",
    "            except KeyError:\n",
    "                deck_counts[deck][pclass] = 0\n",
    "                \n",
    "    df_decks = pd.DataFrame(deck_counts)    \n",
    "    deck_percentages = {}\n",
    "\n",
    "    # Creating a dictionary for every passenger class percentage in every deck\n",
    "    for col in df_decks.columns:\n",
    "        deck_percentages[col] = [(count / df_decks[col].sum()) * 100 for count in df_decks[col]]\n",
    "        \n",
    "    return deck_counts, deck_percentages\n",
    "\n",
    "def display_pclass_dist(percentages):\n",
    "    \n",
    "    df_percentages = pd.DataFrame(percentages).transpose()\n",
    "    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n",
    "    bar_count = np.arange(len(deck_names))  \n",
    "    bar_width = 0.85\n",
    "    \n",
    "    pclass1 = df_percentages[0]\n",
    "    pclass2 = df_percentages[1]\n",
    "    pclass3 = df_percentages[2]\n",
    "\n",
    "df_train['Deck'] = df_train['Deck'].replace(['A', 'B', 'C'], 'ABC')\n",
    "df_test ['Deck']  = df_test['Deck'].replace(['A', 'B', 'C'], 'ABC')\n",
    "\n",
    "df_train['Deck'] = df_train['Deck'].replace(['D', 'E'], 'DE')\n",
    "df_test ['Deck']  = df_test['Deck'].replace(['D', 'E'], 'DE')\n",
    "\n",
    "df_train['Deck'] = df_train['Deck'].replace(['F', 'G'], 'FG')\n",
    "df_test ['Deck']  = df_test['Deck'].replace(['F', 'G'], 'FG')\n",
    "\n",
    "idx_train = df_train[df_train['Deck'] == 'T'].index\n",
    "idx_test = df_test[df_test['Deck'] == 'T'].index\n",
    "\n",
    "df_train.loc[idx_train, 'Deck'] = 'A'\n",
    "df_test.loc[idx_test, 'Deck'] = 'A'\n",
    "\n",
    "df_train['Deck'].value_counts() + df_test['Deck'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90ecb243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            418\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "Deck             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前提: Age は数値\n",
    "df_train['Age'] = pd.to_numeric(df_train['Age'], errors='coerce')\n",
    "df_test['Age']  = pd.to_numeric(df_test['Age'],  errors='coerce')\n",
    "\n",
    "# 学習データの分布で学習側を補完\n",
    "df_train['Age'] = df_train['Age'].fillna(\n",
    "    df_train.groupby(['Sex','Pclass'])['Age'].transform('median')\n",
    ")\n",
    "\n",
    "# 学習データで算出した中央値\n",
    "age_med_train = df_train.groupby(['Sex','Pclass'])['Age'].median().rename('Age_med')\n",
    "\n",
    "# テストへ結合して補完 → 余分列を削除\n",
    "df_test = df_test.join(age_med_train, on=['Sex','Pclass'])\n",
    "df_test['Age'] = df_test['Age'].fillna(df_test['Age_med'])\n",
    "df_test['Age'] = df_test['Age'].fillna(df_train['Age'].median())\n",
    "df_test = df_test.drop(columns=['Age_med'])\n",
    "\n",
    "df_test.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bcf2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family size\n",
    "df_train['Family_Size'] = df_train['SibSp'] + df_train['Parch'] + 1\n",
    "df_test['Family_Size'] = df_test['SibSp'] + df_test['Parch'] + 1\n",
    "# Group family size into bins\n",
    "family_map = {\n",
    "    1: 'Alone',\n",
    "    2: 'Small', 3: 'Small', 4: 'Small',\n",
    "    5: 'Medium', 6: 'Medium',\n",
    "    7: 'Large', 8: 'Large', 11: 'Large'\n",
    "}\n",
    "df_train['Family_Size_Grouped'] = df_train['Family_Size'].map(family_map)\n",
    "df_test['Family_Size_Grouped'] = df_test['Family_Size'].map(family_map)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50acb495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ticket_Frequency\n",
       "1    547\n",
       "2    188\n",
       "3     63\n",
       "4     44\n",
       "7     21\n",
       "6     18\n",
       "5     10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Ticket_Frequency'] = df_train.groupby('Ticket')['Ticket'].transform('count')\n",
    "df_test['Ticket_Frequency'] = df_test.groupby('Ticket')['Ticket'].transform('count')\n",
    "\n",
    "df_train['Ticket_Frequency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07d69bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name\n",
       "Braund, Mr. Owen Harris                     1\n",
       "Boulos, Mr. Hanna                           1\n",
       "Frolicher-Stehli, Mr. Maxmillian            1\n",
       "Gilinski, Mr. Eliezer                       1\n",
       "Murdlin, Mr. Joseph                         1\n",
       "                                           ..\n",
       "Kelly, Miss. Anna Katherine \"Annie Kate\"    1\n",
       "McCoy, Mr. Bernard                          1\n",
       "Johnson, Mr. William Cahoone Jr             1\n",
       "Keane, Miss. Nora A                         1\n",
       "Dooley, Mr. Patrick                         1\n",
       "Name: count, Length: 891, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7046cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Title'] = df_train['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n",
    "df_test ['Title'] = df_test['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]  \n",
    "\n",
    "df_train['Is_Married'] = 0\n",
    "df_test['Is_Married'] = 0\n",
    "\n",
    "df_train['Is_Married'].loc[df_train['Title'] == 'Mrs'] = 1\n",
    "df_test['Is_Married'].loc[df_test['Title'] == 'Mrs'] = 1\n",
    "\n",
    "\n",
    "df_train['Title'] = df_train['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\n",
    "df_test['Title']  = df_test['Title'].replace (['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\n",
    "\n",
    "df_train['Title'] = df_train['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')\n",
    "df_test['Title'] = df_test['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c8df59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_surname(data):    \n",
    "    \n",
    "    families = []\n",
    "    \n",
    "    for i in range(len(data)):        \n",
    "        name = data.iloc[i]\n",
    "\n",
    "        if '(' in name:\n",
    "            name_no_bracket = name.split('(')[0] \n",
    "        else:\n",
    "            name_no_bracket = name\n",
    "            \n",
    "        family = name_no_bracket.split(',')[0]\n",
    "        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n",
    "        \n",
    "        for c in string.punctuation:\n",
    "            family = family.replace(c, '').strip()\n",
    "            \n",
    "        families.append(family)\n",
    "            \n",
    "    return families\n",
    "\n",
    "df_train['Family'] = extract_surname(df_train['Name'])\n",
    "df_test['Family'] = extract_surname(df_test['Name'])\n",
    "\n",
    "dfs = [df_train, df_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da60f2",
   "metadata": {},
   "outputs": [],
   "source": "# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n\n# 修正: グループ化のキー列は集計対象から除外する\ndf_family_survival_rate = df_train.groupby('Family')[['Perished', 'Family_Size']].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')[['Perished', 'Ticket_Frequency']].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]\n\nprint(f\"Family survival rates calculated: {len(family_rates)} families\")\nprint(f\"Ticket survival rates calculated: {len(ticket_rates)} tickets\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_train, df_test, verbose=True):\n",
    "    \"\"\"\n",
    "    データの前処理を実行\n",
    "    \n",
    "    Args:\n",
    "        df_train: トレーニングデータ\n",
    "        df_test: テストデータ\n",
    "        verbose: 進捗表示\n",
    "    \n",
    "    Returns:\n",
    "        X: 特徴量（トレーニング）\n",
    "        y: ターゲット\n",
    "        X_test: 特徴量（テスト）\n",
    "    \"\"\"\n",
    "    # データのコピー\n",
    "    train = df_train.copy()\n",
    "    test = df_test.copy()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"データ前処理開始...\")\n",
    "    \n",
    "    # 1. 欠損値補完\n",
    "    # Age, Fareは全体の平均で補完\n",
    "    age_combined = pd.concat([train['Age'], test['Age']])\n",
    "    fare_combined = pd.concat([train['Fare'], test['Fare']])\n",
    "    \n",
    "    train['Age'].fillna(age_combined.mean(), inplace=True)\n",
    "    test['Age'].fillna(age_combined.mean(), inplace=True)\n",
    "    \n",
    "    med_fare_train = df_train.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n",
    "    med_fare_test = df_test.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n",
    "# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\n",
    "    df_train['Fare'] = df_train['Fare'].fillna(med_fare_train)\n",
    "    df_test['Fare'] = df_test['Fare'].fillna(med_fare_test)\n",
    "    \n",
    "    # Embarkedは最頻値'S'で補完\n",
    "    train['Embarked'].fillna('S', inplace=True)\n",
    "    test['Embarked'].fillna('S', inplace=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"  ✓ 欠損値補完完了\")\n",
    "    \n",
    "    # 2. 不要な列を削除\n",
    "    drop_cols = ['Cabin', 'Name', 'Ticket']\n",
    "    train.drop(drop_cols, axis=1, inplace=True)\n",
    "    test.drop(drop_cols, axis=1, inplace=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"  ✓ 不要な列を削除\")\n",
    "    \n",
    "    # 3. Sex列のエンコーディング\n",
    "    train['Sex'] = train['Sex'].map({'male': 0, 'female': 1})\n",
    "    test['Sex'] = test['Sex'].map({'male': 0, 'female': 1})\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"  ✓ Sex列をエンコーディング\")\n",
    "    \n",
    "    # 4. Embarked列のOne-Hot Encoding\n",
    "    embarked_combined = pd.concat([train['Embarked'], test['Embarked']])\n",
    "    embarked_ohe = pd.get_dummies(embarked_combined, prefix='Embarked')\n",
    "    \n",
    "    train = pd.concat([train, embarked_ohe[:len(train)]], axis=1)\n",
    "    test = pd.concat([test, embarked_ohe[len(train):]], axis=1)\n",
    "    \n",
    "    train.drop('Embarked', axis=1, inplace=True)\n",
    "    test.drop('Embarked', axis=1, inplace=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"  ✓ Embarked列をOne-Hot Encoding\")\n",
    "    \n",
    "    # 5. 特徴量とターゲットに分割\n",
    "    X = train.drop(['PassengerId', 'Perished'], axis=1).values\n",
    "    y = train['Perished'].values\n",
    "    X_test = test.drop('PassengerId', axis=1).values\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  ✓ 特徴量形状: X={X.shape}, X_test={X_test.shape}\")\n",
    "        print(\"データ前処理完了!\")\n",
    "    \n",
    "    return X, y, X_test\n",
    "\n",
    "# 前処理実行\n",
    "X, y, X_test = preprocess_data(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングデータと検証データに分割\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Valid set: {X_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## 4. モデルトレーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline_models(X_train, y_train, X_valid, y_valid):\n",
    "    \"\"\"\n",
    "    複数のベースラインモデルをトレーニング\n",
    "    \n",
    "    Returns:\n",
    "        dict: トレーニング済みモデルの辞書\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Baseline Models Training\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # RandomForest\n",
    "    print(\"\\n[1/6] Random Forest...\")\n",
    "    rfc = RandomForestClassifier(\n",
    "        max_depth=10, min_samples_leaf=1, n_estimators=100,\n",
    "        n_jobs=N_JOBS, random_state=RANDOM_STATE\n",
    "    )\n",
    "    rfc.fit(X_train, y_train)\n",
    "    models['RandomForest'] = rfc\n",
    "    print(f\"  Train: {rfc.score(X_train, y_train):.3f}, Valid: {rfc.score(X_valid, y_valid):.3f}\")\n",
    "    \n",
    "    # Logistic Regression\n",
    "    print(\"\\n[2/6] Logistic Regression...\")\n",
    "    lr = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    models['LogisticReg'] = lr\n",
    "    print(f\"  Train: {lr.score(X_train, y_train):.3f}, Valid: {lr.score(X_valid, y_valid):.3f}\")\n",
    "    \n",
    "    # MLP\n",
    "    print(\"\\n[3/6] MLP Classifier...\")\n",
    "    mlpc = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 100, 10),\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    mlpc.fit(X_train, y_train)\n",
    "    models['MLP'] = mlpc\n",
    "    print(f\"  Train: {mlpc.score(X_train, y_train):.3f}, Valid: {mlpc.score(X_valid, y_valid):.3f}\")\n",
    "    \n",
    "    # CatBoost\n",
    "    print(\"\\n[4/6] CatBoost...\")\n",
    "    cbc = CatBoostClassifier(\n",
    "        iterations=100, depth=6, learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE, verbose=0\n",
    "    )\n",
    "    cbc.fit(X_train, y_train)\n",
    "    models['CatBoost'] = cbc\n",
    "    print(f\"  Train: {cbc.score(X_train, y_train):.3f}, Valid: {cbc.score(X_valid, y_valid):.3f}\")\n",
    "    \n",
    "    # XGBoost\n",
    "    print(\"\\n[5/6] XGBoost...\")\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE, eval_metric=\"logloss\"\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    models['XGBoost'] = xgb\n",
    "    print(f\"  Train: {xgb.score(X_train, y_train):.3f}, Valid: {xgb.score(X_valid, y_valid):.3f}\")\n",
    "    \n",
    "    # LightGBM\n",
    "    print(\"\\n[6/6] LightGBM...\")\n",
    "    lgb = LGBMClassifier(\n",
    "        n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE, verbose=-1\n",
    "    )\n",
    "    lgb.fit(X_train, y_train)\n",
    "    models['LightGBM'] = lgb\n",
    "    print(f\"  Train: {lgb.score(X_train, y_train):.3f}, Valid: {lgb.score(X_valid, y_valid):.3f}\")\n",
    "    \n",
    "    print(\"\\nベースラインモデルのトレーニング完了!\")\n",
    "    return models\n",
    "\n",
    "# ベースラインモデルのトレーニング\n",
    "baseline_models = train_baseline_models(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. ハイパーパラメータチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameter_tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_models(X_train, y_train, X_valid, y_valid):\n",
    "    \"\"\"\n",
    "    GridSearchでハイパーパラメータチューニング\n",
    "    \n",
    "    Returns:\n",
    "        dict: チューニング済みモデルの辞書\n",
    "    \"\"\"\n",
    "    tuned_models = {}\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Hyperparameter Tuning\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # CatBoost\n",
    "    print(\"\\n[1/3] CatBoost...\")\n",
    "    cbc_params = {\n",
    "        \"iterations\": [100, 200],\n",
    "        \"depth\": [4, 6, 8],\n",
    "        \"learning_rate\": [0.05, 0.1]\n",
    "    }\n",
    "    cbc_gs = GridSearchCV(\n",
    "        CatBoostClassifier(random_state=RANDOM_STATE, verbose=0),\n",
    "        cbc_params, cv=5, scoring=\"accuracy\", n_jobs=N_JOBS\n",
    "    )\n",
    "    cbc_gs.fit(X_train, y_train)\n",
    "    tuned_models['CatBoost'] = cbc_gs\n",
    "    print(f\"  Best params: {cbc_gs.best_params_}\")\n",
    "    print(f\"  CV Score: {cbc_gs.best_score_:.3f}\")\n",
    "    print(f\"  Valid Score: {cbc_gs.score(X_valid, y_valid):.3f}\")\n",
    "    \n",
    "    # XGBoost\n",
    "    print(\"\\n[2/3] XGBoost...\")\n",
    "    xgb_params = {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"max_depth\": [4, 6, 8],\n",
    "        \"learning_rate\": [0.05, 0.1]\n",
    "    }\n",
    "    xgb_gs = GridSearchCV(\n",
    "        XGBClassifier(random_state=RANDOM_STATE, eval_metric=\"logloss\"),\n",
    "        xgb_params, cv=5, scoring=\"accuracy\", n_jobs=N_JOBS\n",
    "    )\n",
    "    xgb_gs.fit(X_train, y_train)\n",
    "    tuned_models['XGBoost'] = xgb_gs\n",
    "    print(f\"  Best params: {xgb_gs.best_params_}\")\n",
    "    print(f\"  CV Score: {xgb_gs.best_score_:.3f}\")\n",
    "    print(f\"  Valid Score: {xgb_gs.score(X_valid, y_valid):.3f}\")\n",
    "    \n",
    "    # LightGBM\n",
    "    print(\"\\n[3/3] LightGBM...\")\n",
    "    lgb_params = {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"max_depth\": [4, 6, 8],\n",
    "        \"learning_rate\": [0.05, 0.1]\n",
    "    }\n",
    "    lgb_gs = GridSearchCV(\n",
    "        LGBMClassifier(random_state=RANDOM_STATE, verbose=-1),\n",
    "        lgb_params, cv=5, scoring=\"accuracy\", n_jobs=1\n",
    "    )\n",
    "    lgb_gs.fit(X_train, y_train)\n",
    "    tuned_models['LightGBM'] = lgb_gs\n",
    "    print(f\"  Best params: {lgb_gs.best_params_}\")\n",
    "    print(f\"  CV Score: {lgb_gs.best_score_:.3f}\")\n",
    "    print(f\"  Valid Score: {lgb_gs.score(X_valid, y_valid):.3f}\")\n",
    "    \n",
    "    print(\"\\nハイパーパラメータチューニング完了!\")\n",
    "    return tuned_models\n",
    "\n",
    "# ハイパーパラメータチューニング実行\n",
    "tuned_models = tune_models(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル性能比較\n",
    "def compare_models(baseline_models, tuned_models, X_train, y_train, X_valid, y_valid):\n",
    "    \"\"\"\n",
    "    全モデルの性能を比較\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # ベースラインモデル\n",
    "    for name, model in baseline_models.items():\n",
    "        if name not in tuned_models:  # チューニングされていないモデルのみ\n",
    "            train_score = model.score(X_train, y_train)\n",
    "            valid_score = model.score(X_valid, y_valid)\n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'Train Score': train_score,\n",
    "                'Valid Score': valid_score,\n",
    "                'Overfit': train_score - valid_score\n",
    "            })\n",
    "    \n",
    "    # チューニング済みモデル\n",
    "    for name, model in tuned_models.items():\n",
    "        train_score = model.best_estimator_.score(X_train, y_train)\n",
    "        valid_score = model.score(X_valid, y_valid)\n",
    "        results.append({\n",
    "            'Model': f\"{name} (Tuned)\",\n",
    "            'Train Score': train_score,\n",
    "            'Valid Score': valid_score,\n",
    "            'Overfit': train_score - valid_score\n",
    "        })\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results = df_results.round(3)\n",
    "    df_results = df_results.sort_values('Valid Score', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Model Performance Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df_results.to_string(index=False))\n",
    "    \n",
    "    best_model = df_results.iloc[0]\n",
    "    print(f\"\\nBest Model: {best_model['Model']}\")\n",
    "    print(f\"Valid Score: {best_model['Valid Score']:.3f}\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "model_comparison = compare_models(\n",
    "    baseline_models, tuned_models, X_train, y_train, X_valid, y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "## 6. Wandb統合（オプション）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wandb_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb統合（オプション）\n",
    "USE_WANDB = False  # Trueに変更してwandbを有効化\n",
    "\n",
    "if USE_WANDB:\n",
    "    try:\n",
    "        import wandb\n",
    "        \n",
    "        # 環境変数からAPIキーを読み込む（セキュアな方法）\n",
    "        # os.environ[\"WANDB_API_KEY\"] = \"your-api-key\"  # または.envファイルから読み込み\n",
    "        \n",
    "        wandb.login(timeout=30)\n",
    "        \n",
    "        run = wandb.init(\n",
    "            project=\"titanic-classification\",\n",
    "            name=\"titanic-refactored\",\n",
    "            settings=wandb.Settings(start_method=\"fork\"),\n",
    "            config={\n",
    "                \"dataset\": \"Titanic\",\n",
    "                \"test_size\": TEST_SIZE,\n",
    "                \"random_state\": RANDOM_STATE\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # モデル性能をログ\n",
    "        for _, row in model_comparison.iterrows():\n",
    "            wandb.log({\n",
    "                f\"{row['Model']}_train\": row['Train Score'],\n",
    "                f\"{row['Model']}_valid\": row['Valid Score'],\n",
    "                f\"{row['Model']}_overfit\": row['Overfit']\n",
    "            })\n",
    "        \n",
    "        wandb.log({\"model_comparison\": wandb.Table(dataframe=model_comparison)})\n",
    "        \n",
    "        print(\"wandb initialized successfully!\")\n",
    "        print(f\"Run URL: {wandb.run.get_url()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"wandb初期化失敗: {e}\")\n",
    "        print(\"wandbなしで続行します...\")\n",
    "        USE_WANDB = False\n",
    "else:\n",
    "    print(\"wandbは無効です。USE_WANDB=Trueに設定して有効化してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7",
   "metadata": {},
   "source": [
    "## 7. アンサンブルモデルと予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# アンサンブルモデルの作成\n",
    "def create_ensemble(baseline_models, tuned_models):\n",
    "    \"\"\"\n",
    "    Voting Classifierでアンサンブルモデルを作成\n",
    "    \"\"\"\n",
    "    estimators = [\n",
    "        ('rfc', baseline_models['RandomForest']),\n",
    "        ('lr', baseline_models['LogisticReg']),\n",
    "        ('mlpc', baseline_models['MLP']),\n",
    "        ('cbc', tuned_models['CatBoost'].best_estimator_),\n",
    "        ('xgb', tuned_models['XGBoost'].best_estimator_),\n",
    "        ('lgb', tuned_models['LightGBM'].best_estimator_)\n",
    "    ]\n",
    "    \n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=estimators,\n",
    "        voting='soft',\n",
    "        n_jobs=N_JOBS\n",
    "    )\n",
    "    \n",
    "    return ensemble\n",
    "\n",
    "# アンサンブルモデルのトレーニング\n",
    "print(\"=\" * 60)\n",
    "print(\"Ensemble Model Training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ensemble = create_ensemble(baseline_models, tuned_models)\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "train_score = ensemble.score(X_train, y_train)\n",
    "valid_score = ensemble.score(X_valid, y_valid)\n",
    "\n",
    "print(f\"\\nEnsemble Model (Soft Voting)\")\n",
    "print(f\"  Train Score: {train_score:.3f}\")\n",
    "print(f\"  Valid Score: {valid_score:.3f}\")\n",
    "print(f\"  Overfit: {train_score - valid_score:.3f}\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\n",
    "        \"ensemble_train\": train_score,\n",
    "        \"ensemble_valid\": valid_score,\n",
    "        \"ensemble_overfit\": train_score - valid_score\n",
    "    })\n",
    "    wandb.run.summary[\"final_model\"] = \"Ensemble (Soft Voting)\"\n",
    "    wandb.run.summary[\"final_valid_score\"] = valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測の生成\n",
    "print(\"\\n予測を生成中...\")\n",
    "predictions = ensemble.predict(X_test)\n",
    "\n",
    "# 提出ファイルの作成\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': df_test['PassengerId'],\n",
    "    'Perished': predictions\n",
    "})\n",
    "\n",
    "# 出力ディレクトリの作成\n",
    "os.makedirs('../output', exist_ok=True)\n",
    "output_path = '../output/submission_refactored.csv'\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n提出ファイルを保存: {output_path}\")\n",
    "print(f\"総予測数: {len(submission)}\")\n",
    "print(f\"生存予測: {(predictions == 0).sum()} ({(predictions == 0).sum()/len(predictions)*100:.1f}%)\")\n",
    "print(f\"死亡予測: {(predictions == 1).sum()} ({(predictions == 1).sum()/len(predictions)*100:.1f}%)\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\n",
    "        \"total_predictions\": len(submission),\n",
    "        \"survived_count\": (predictions == 0).sum(),\n",
    "        \"died_count\": (predictions == 1).sum(),\n",
    "        \"survival_rate\": (predictions == 0).sum() / len(predictions)\n",
    "    })\n",
    "    \n",
    "    artifact = wandb.Artifact('submission', type='predictions')\n",
    "    artifact.add_file(output_path)\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wandb_finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandbセッション終了\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n",
    "    print(\"wandb run finished successfully!\")\n",
    "    print(f\"View results at: https://wandb.ai/{wandb.run.entity}/{wandb.run.project}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"すべての処理が完了しました!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
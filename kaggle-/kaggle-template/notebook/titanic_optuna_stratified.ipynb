{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction - Optuna + Stratified K-Fold\n",
    "\n",
    "このノートブックは、Optunaによるハイパーパラメータ最適化とStratified K-Foldクロスバリデーションを使用します。\n",
    "\n",
    "## 目次\n",
    "1. セットアップとデータ読み込み\n",
    "2. 特徴量エンジニアリング\n",
    "3. Optunaによるハイパーパラメータチューニング\n",
    "4. Stratified K-Fold学習\n",
    "5. アンサンブル予測と提出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. セットアップとデータ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "import string\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgbm\n",
    "\n",
    "# TensorFlow/Keras for CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# SHAP for model interpretation\n",
    "import shap\n",
    "\n",
    "# Weights & Biases for experiment tracking\n",
    "import wandb\n",
    "\n",
    "# 定数設定\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 2\n",
    "N_TRIALS = 100  # Optunaの試行回数（より多くのパラメータを探索）\n",
    "TIMEOUT = 600  # Optunaのタイムアウト（秒）- より長く設定\n",
    "\n",
    "# wandb設定\n",
    "WANDB_PROJECT = \"titanic-classification\"\n",
    "WANDB_API_KEY = \"645b90cb2db844ae6d87767f0e414fac7daf7461\"\n",
    "\n",
    "# 乱数シード固定\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "print(\"ライブラリのインポート完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_train['Perished'].value_counts(normalize=True))\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb初期化\n",
    "import sys\n",
    "sys.path.append('../notebook')\n",
    "\n",
    "\n",
    "# wandbにログイン\n",
    "wandb.login(key=\"645b90cb2db844ae6d87767f0e414fac7daf7461\")\n",
    "\n",
    "# wandb runを開始\n",
    "run = wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    name=\"titanic-5model-ensemble\",\n",
    "    config={\n",
    "        \"n_folds\": N_FOLDS,\n",
    "        \"n_trials\": N_TRIALS,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"models\": [\"CNN\", \"RandomForest\", \"CatBoost\", \"XGBoost\", \"LightGBM\"],\n",
    "        \"features\": X.columns.tolist() if 'X' in dir() else []\n",
    "    },\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Wandb initialized: {WANDB_PROJECT}\")\n",
    "print(f\"  Run name: titanic-5model-ensemble\")\n",
    "print(f\"  Dashboard: {run.get_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_surname(data):\n",
    "    \"\"\"名前から姓を抽出\"\"\"\n",
    "    families = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        name = data.iloc[i]\n",
    "        if '(' in name:\n",
    "            name_no_bracket = name.split('(')[0]\n",
    "        else:\n",
    "            name_no_bracket = name\n",
    "        \n",
    "        family = name_no_bracket.split(',')[0]\n",
    "        \n",
    "        for c in string.punctuation:\n",
    "            family = family.replace(c, '').strip()\n",
    "        \n",
    "        families.append(family)\n",
    "    \n",
    "    return families\n",
    "\n",
    "\n",
    "def create_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    特徴量エンジニアリングを実行\n",
    "    \n",
    "    Returns:\n",
    "        train_df, test_df: 特徴量を追加したDataFrame\n",
    "    \"\"\"\n",
    "    train = df_train.copy()\n",
    "    test = df_test.copy()\n",
    "    \n",
    "    # 全データを結合して特徴量作成\n",
    "    all_data = pd.concat([train, test], axis=0, sort=False).reset_index(drop=True)\n",
    "    \n",
    "    print(\"特徴量エンジニアリング開始...\")\n",
    "    \n",
    "    # 1. Title（敬称）抽出\n",
    "    all_data['Title'] = all_data['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n",
    "    \n",
    "    # Titleをグループ化\n",
    "    all_data['Title'] = all_data['Title'].replace(\n",
    "        ['Miss', 'Mrs', 'Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms'\n",
    "    )\n",
    "    all_data['Title'] = all_data['Title'].replace(\n",
    "        ['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy'\n",
    "    )\n",
    "    \n",
    "    # 2. 結婚フラグ\n",
    "    all_data['Is_Married'] = 0\n",
    "    all_data.loc[all_data['Title'] == 'Mrs', 'Is_Married'] = 1\n",
    "    \n",
    "    # 3. Family（姓）\n",
    "    all_data['Family'] = extract_surname(all_data['Name'])\n",
    "    \n",
    "    # 4. Family_Size（家族サイズ）\n",
    "    all_data['Family_Size'] = all_data['SibSp'] + all_data['Parch'] + 1\n",
    "    \n",
    "    # 5. Family_Size_Grouped（家族サイズのビン化）\n",
    "    family_map = {\n",
    "        1: 'Alone',\n",
    "        2: 'Small', 3: 'Small', 4: 'Small',\n",
    "        5: 'Medium', 6: 'Medium',\n",
    "        7: 'Large', 8: 'Large', 11: 'Large'\n",
    "    }\n",
    "    all_data['Family_Size_Grouped'] = all_data['Family_Size'].map(family_map)\n",
    "    \n",
    "    # 6. Ticket_Frequency（チケット頻度）\n",
    "    all_data['Ticket_Frequency'] = all_data.groupby('Ticket')['Ticket'].transform('count')\n",
    "    \n",
    "    # 7. Deck（デッキ）\n",
    "    all_data['Deck'] = all_data['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n",
    "    \n",
    "    # デッキをグループ化\n",
    "    all_data['Deck'] = all_data['Deck'].replace(['A', 'B', 'C'], 'ABC')\n",
    "    all_data['Deck'] = all_data['Deck'].replace(['D', 'E'], 'DE')\n",
    "    all_data['Deck'] = all_data['Deck'].replace(['F', 'G'], 'FG')\n",
    "    all_data['Deck'] = all_data['Deck'].replace(['T'], 'M')\n",
    "    \n",
    "    # 8. Age補完と離散化\n",
    "    all_data['Age'] = pd.to_numeric(all_data['Age'], errors='coerce')\n",
    "    \n",
    "    # Age を Sex x Pclass の中央値で補完\n",
    "    age_by_pclass_sex = all_data.groupby(['Sex', 'Pclass'])['Age'].median()\n",
    "    \n",
    "    for pclass in [1, 2, 3]:\n",
    "        for sex in ['male', 'female']:\n",
    "            mask = (all_data['Age'].isnull()) & (all_data['Pclass'] == pclass) & (all_data['Sex'] == sex)\n",
    "            all_data.loc[mask, 'Age'] = age_by_pclass_sex.loc[(sex, pclass)]\n",
    "    \n",
    "    # Ageをビン化\n",
    "    all_data['Age_Band'] = pd.cut(all_data['Age'], bins=[0, 12, 18, 30, 50, 80], \n",
    "                                    labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])\n",
    "    \n",
    "    # 9. Fare補完と離散化\n",
    "    all_data['Fare'] = all_data['Fare'].fillna(all_data['Fare'].median())\n",
    "    all_data['Fare_Band'] = pd.qcut(all_data['Fare'], q=4, labels=['Low', 'Medium', 'High', 'Very_High'], duplicates='drop')\n",
    "    \n",
    "    # 10. Embarked補完\n",
    "    all_data['Embarked'] = all_data['Embarked'].fillna('S')\n",
    "    \n",
    "    # 11. Family_Survival（家族生存率） - 訓練データのみから計算\n",
    "    train_idx = ~all_data['Perished'].isna()\n",
    "    \n",
    "    # 家族生存率を計算\n",
    "    family_survival = all_data[train_idx].groupby('Family')['Perished'].transform('mean')\n",
    "    family_survival_dict = all_data[train_idx].groupby('Family')['Perished'].mean().to_dict()\n",
    "    \n",
    "    all_data['Family_Survival'] = all_data['Family'].map(family_survival_dict)\n",
    "    all_data['Family_Survival'] = all_data['Family_Survival'].fillna(0.5)  # 不明は0.5\n",
    "    \n",
    "    # 12. Ticket_Survival（チケット生存率）\n",
    "    ticket_survival_dict = all_data[train_idx].groupby('Ticket')['Perished'].mean().to_dict()\n",
    "    all_data['Ticket_Survival'] = all_data['Ticket'].map(ticket_survival_dict)\n",
    "    all_data['Ticket_Survival'] = all_data['Ticket_Survival'].fillna(0.5)\n",
    "    \n",
    "    # 13. Sex x Pclass 交互作用\n",
    "    all_data['Sex_Pclass'] = all_data['Sex'] + '_' + all_data['Pclass'].astype(str)\n",
    "    \n",
    "    print(f\"  ✓ 特徴量作成完了: {all_data.shape[1]} columns\")\n",
    "    \n",
    "    # 訓練データとテストデータに分割\n",
    "    train_processed = all_data[train_idx].reset_index(drop=True)\n",
    "    test_processed = all_data[~train_idx].reset_index(drop=True)\n",
    "    \n",
    "    return train_processed, test_processed\n",
    "\n",
    "\n",
    "# 特徴量作成\n",
    "train_df, test_df = create_features(df_train, df_test)\n",
    "\n",
    "print(f\"\\nTrain shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nCreated features: {train_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df, test_df):\n",
    "    \"\"\"\n",
    "    データを機械学習モデル用に準備\n",
    "    \"\"\"\n",
    "    # 使用する特徴量を選択\n",
    "    feature_cols = [\n",
    "        'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
    "        'Embarked', 'Title', 'Is_Married', 'Family_Size',\n",
    "        'Family_Size_Grouped', 'Ticket_Frequency', 'Deck',\n",
    "        'Age_Band', 'Fare_Band', 'Family_Survival', 'Ticket_Survival',\n",
    "        'Sex_Pclass'\n",
    "    ]\n",
    "    \n",
    "    train = train_df[feature_cols + ['Perished']].copy()\n",
    "    test = test_df[feature_cols].copy()\n",
    "    \n",
    "    # カテゴリカル変数をエンコード\n",
    "    cat_features = ['Sex', 'Embarked', 'Title', 'Family_Size_Grouped', 'Deck', 'Age_Band', 'Fare_Band', 'Sex_Pclass']\n",
    "    \n",
    "    le_dict = {}\n",
    "    for col in cat_features:\n",
    "        le = LabelEncoder()\n",
    "        # 訓練データとテストデータを結合してfit\n",
    "        all_values = pd.concat([train[col], test[col]], axis=0)\n",
    "        le.fit(all_values.astype(str))\n",
    "        \n",
    "        train[col] = le.transform(train[col].astype(str))\n",
    "        test[col] = le.transform(test[col].astype(str))\n",
    "        \n",
    "        le_dict[col] = le\n",
    "    \n",
    "    # 特徴量とターゲットに分割\n",
    "    X = train.drop('Perished', axis=1)\n",
    "    y = train['Perished']\n",
    "    X_test = test\n",
    "    \n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"\\nFeatures: {X.columns.tolist()}\")\n",
    "    \n",
    "    return X, y, X_test, cat_features\n",
    "\n",
    "\n",
    "X, y, X_test, cat_features = prepare_data(train_df, test_df)\n",
    "\n",
    "print(f\"\\nCategorical features: {cat_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optunaによるハイパーパラメータチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna用のObjective関数を定義 - 詳細なハイパーパラメータチューニング\n",
    "\n",
    "def objective_cnn(trial, X, y, n_folds=2):\n",
    "    \"\"\"1D CNN用のObjective関数 - 表形式データ用\"\"\"\n",
    "    \n",
    "    # ハイパーパラメータ\n",
    "    params = {\n",
    "        'n_conv_layers': trial.suggest_int('n_conv_layers', 1, 3),\n",
    "        'filters_1': trial.suggest_int('filters_1', 32, 256),\n",
    "        'kernel_size_1': trial.suggest_int('kernel_size_1', 2, 5),\n",
    "        'dropout_conv': trial.suggest_float('dropout_conv', 0.1, 0.5),\n",
    "        \n",
    "        'n_dense_layers': trial.suggest_int('n_dense_layers', 1, 3),\n",
    "        'dense_units_1': trial.suggest_int('dense_units_1', 32, 256),\n",
    "        'dropout_dense': trial.suggest_float('dropout_dense', 0.2, 0.6),\n",
    "        \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "        'epochs': trial.suggest_int('epochs', 50, 200)\n",
    "    }\n",
    "    \n",
    "    # 追加のCNN層パラメータ\n",
    "    if params['n_conv_layers'] >= 2:\n",
    "        params['filters_2'] = trial.suggest_int('filters_2', 32, 256)\n",
    "        params['kernel_size_2'] = trial.suggest_int('kernel_size_2', 2, 5)\n",
    "    if params['n_conv_layers'] >= 3:\n",
    "        params['filters_3'] = trial.suggest_int('filters_3', 32, 256)\n",
    "        params['kernel_size_3'] = trial.suggest_int('kernel_size_3', 2, 5)\n",
    "    \n",
    "    # 追加のDense層パラメータ\n",
    "    if params['n_dense_layers'] >= 2:\n",
    "        params['dense_units_2'] = trial.suggest_int('dense_units_2', 32, 256)\n",
    "    if params['n_dense_layers'] >= 3:\n",
    "        params['dense_units_3'] = trial.suggest_int('dense_units_3', 32, 128)\n",
    "    \n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    \n",
    "    # データ正規化用\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        # データ正規化\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_valid_scaled = scaler.transform(X_valid)\n",
    "        \n",
    "        # CNNのため3次元に変換 (samples, timesteps, features)\n",
    "        X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "        X_valid_cnn = X_valid_scaled.reshape(X_valid_scaled.shape[0], X_valid_scaled.shape[1], 1)\n",
    "        \n",
    "        # モデル構築\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        # Conv層1\n",
    "        model.add(layers.Conv1D(\n",
    "            filters=params['filters_1'],\n",
    "            kernel_size=params['kernel_size_1'],\n",
    "            activation='relu',\n",
    "            input_shape=(X_train_cnn.shape[1], 1)\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(params['dropout_conv']))\n",
    "        \n",
    "        # Conv層2\n",
    "        if params['n_conv_layers'] >= 2:\n",
    "            model.add(layers.Conv1D(\n",
    "                filters=params['filters_2'],\n",
    "                kernel_size=params['kernel_size_2'],\n",
    "                activation='relu'\n",
    "            ))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(params['dropout_conv']))\n",
    "        \n",
    "        # Conv層3\n",
    "        if params['n_conv_layers'] >= 3:\n",
    "            model.add(layers.Conv1D(\n",
    "                filters=params['filters_3'],\n",
    "                kernel_size=params['kernel_size_3'],\n",
    "                activation='relu'\n",
    "            ))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(params['dropout_conv']))\n",
    "        \n",
    "        model.add(layers.GlobalMaxPooling1D())\n",
    "        \n",
    "        # Dense層1\n",
    "        model.add(layers.Dense(params['dense_units_1'], activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(params['dropout_dense']))\n",
    "        \n",
    "        # Dense層2\n",
    "        if params['n_dense_layers'] >= 2:\n",
    "            model.add(layers.Dense(params['dense_units_2'], activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(params['dropout_dense']))\n",
    "        \n",
    "        # Dense層3\n",
    "        if params['n_dense_layers'] >= 3:\n",
    "            model.add(layers.Dense(params['dense_units_3'], activation='relu'))\n",
    "            model.add(layers.Dropout(params['dropout_dense']))\n",
    "        \n",
    "        # 出力層\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        # コンパイル\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=params['learning_rate']),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Early Stopping\n",
    "        early_stop = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # 学習\n",
    "        model.fit(\n",
    "            X_train_cnn, y_train,\n",
    "            validation_data=(X_valid_cnn, y_valid),\n",
    "            epochs=params['epochs'],\n",
    "            batch_size=params['batch_size'],\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # 予測\n",
    "        preds_proba = model.predict(X_valid_cnn, verbose=0).flatten()\n",
    "        preds = (preds_proba >= 0.5).astype(int)\n",
    "        score = accuracy_score(y_valid, preds)\n",
    "        scores.append(score)\n",
    "        \n",
    "        # メモリ解放\n",
    "        tf.keras.backend.clear_session()\n",
    "        del model\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def objective_randomforest(trial, X, y, n_folds=2):\n",
    "    \"\"\"RandomForest用のObjective関数 - 詳細版\"\"\"\n",
    "    params = {\n",
    "        # 基本パラメータ\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        \n",
    "        # 特徴量サンプリング\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 100),\n",
    "        \n",
    "        # ブートストラップとサンプリング\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'max_samples': trial.suggest_float('max_samples', 0.5, 1.0) if trial.params.get('bootstrap', True) else None,\n",
    "        \n",
    "        # 不純度と分割\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "        'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.5),\n",
    "        \n",
    "        # その他\n",
    "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample', None]),\n",
    "        'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1),\n",
    "        \n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # bootstrap=Falseの場合はmax_samplesを削除\n",
    "    if not params['bootstrap']:\n",
    "        params.pop('max_samples', None)\n",
    "    \n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        model = RandomForestClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        preds = model.predict(X_valid)\n",
    "        score = accuracy_score(y_valid, preds)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def objective_catboost(trial, X, y, n_folds=2):\n",
    "    \"\"\"CatBoost用のObjective関数 - 詳細版\"\"\"\n",
    "    params = {\n",
    "        # 基本パラメータ\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 1, 10),\n",
    "        \n",
    "        # 正則化パラメータ\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 10.0, log=True),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-9, 10.0, log=True),\n",
    "        \n",
    "        # バギングとブースティング\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.05, 1.0),\n",
    "        \n",
    "        # リーフ関連\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "        'max_leaves': trial.suggest_int('max_leaves', 16, 64),\n",
    "        \n",
    "        # その他\n",
    "        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 10),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': 0,\n",
    "        'task_type': 'CPU'\n",
    "    }\n",
    "    \n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=50, verbose=0)\n",
    "        \n",
    "        preds = model.predict(X_valid)\n",
    "        score = accuracy_score(y_valid, preds)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def objective_xgboost(trial, X, y, n_folds=2):\n",
    "    \"\"\"XGBoost用のObjective関数 - 詳細版\"\"\"\n",
    "    params = {\n",
    "        # 基本パラメータ\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),  # eta\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        \n",
    "        # ツリー構造パラメータ\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-9, 1.0, log=True),\n",
    "        'max_delta_step': trial.suggest_int('max_delta_step', 0, 10),\n",
    "        \n",
    "        # サンプリングパラメータ\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.5, 1.0),\n",
    "        \n",
    "        # 正則化パラメータ\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 10.0, log=True),  # L1正則化\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 10.0, log=True),  # L2正則化\n",
    "        \n",
    "        # その他\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.5, 2.0),\n",
    "        'max_bin': trial.suggest_int('max_bin', 128, 512),\n",
    "        \n",
    "        'random_state': RANDOM_STATE,\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=50, verbose=0)\n",
    "        \n",
    "        preds = model.predict(X_valid)\n",
    "        score = accuracy_score(y_valid, preds)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def objective_lightgbm(trial, X, y, n_folds=5):\n",
    "    \"\"\"LightGBM用のObjective関数 - 詳細版\"\"\"\n",
    "    params = {\n",
    "        # 基本パラメータ\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        \n",
    "        # リーフ関連パラメータ\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1e-5, 1e2, log=True),\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 1.0),\n",
    "        \n",
    "        # サンプリングパラメータ\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'subsample_freq': trial.suggest_int('subsample_freq', 0, 10),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        \n",
    "        # 正則化パラメータ\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 10.0, log=True),  # L1正則化\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 10.0, log=True),  # L2正則化\n",
    "        \n",
    "        # カテゴリカル関連\n",
    "        'max_cat_threshold': trial.suggest_int('max_cat_threshold', 10, 100),\n",
    "        'cat_smooth': trial.suggest_float('cat_smooth', 1.0, 100.0),\n",
    "        \n",
    "        # その他\n",
    "        'max_bin': trial.suggest_int('max_bin', 128, 512),\n",
    "        'path_smooth': trial.suggest_float('path_smooth', 0.0, 1.0),\n",
    "        \n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': -1,\n",
    "        'boosting_type': 'gbdt'\n",
    "    }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[lgbm.early_stopping(50), lgbm.log_evaluation(0)])\n",
    "        \n",
    "        preds = model.predict(X_valid)\n",
    "        score = accuracy_score(y_valid, preds)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"Objective functions defined with extensive hyperparameters (including CNN and RandomForest).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNの最適化\n",
    "print(\"=\"*60)\n",
    "print(\"CNN (1D Convolutional Neural Network) Hyperparameter Optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study_cnn = optuna.create_study(direction='maximize', sampler=TPESampler(seed=RANDOM_STATE))\n",
    "study_cnn.optimize(lambda trial: objective_cnn(trial, X, y, N_FOLDS), \n",
    "                   n_trials=N_TRIALS, timeout=TIMEOUT, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest CNN Score: {study_cnn.best_value:.4f}\")\n",
    "print(f\"Best CNN Params: {study_cnn.best_params}\")\n",
    "\n",
    "best_params_cnn = study_cnn.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoostの最適化\n",
    "print(\"=\"*60)\n",
    "print(\"CatBoost Hyperparameter Optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study_catboost = optuna.create_study(direction='maximize', sampler=TPESampler(seed=RANDOM_STATE))\n",
    "study_catboost.optimize(lambda trial: objective_catboost(trial, X, y, N_FOLDS), \n",
    "                        n_trials=N_TRIALS, timeout=TIMEOUT, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest CatBoost Score: {study_catboost.best_value:.4f}\")\n",
    "print(f\"Best CatBoost Params: {study_catboost.best_params}\")\n",
    "\n",
    "best_params_catboost = study_catboost.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoostの最適化\n",
    "print(\"=\"*60)\n",
    "print(\"XGBoost Hyperparameter Optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study_xgboost = optuna.create_study(direction='maximize', sampler=TPESampler(seed=RANDOM_STATE))\n",
    "study_xgboost.optimize(lambda trial: objective_xgboost(trial, X, y, N_FOLDS), \n",
    "                       n_trials=N_TRIALS, timeout=TIMEOUT, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest XGBoost Score: {study_xgboost.best_value:.4f}\")\n",
    "print(f\"Best XGBoost Params: {study_xgboost.best_params}\")\n",
    "\n",
    "best_params_xgboost = study_xgboost.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stratified K-Fold学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(params, input_shape):\n",
    "    \"\"\"CNNモデルを構築する補助関数\"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Conv層1\n",
    "    model.add(layers.Conv1D(\n",
    "        filters=params['filters_1'],\n",
    "        kernel_size=params['kernel_size_1'],\n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(params['dropout_conv']))\n",
    "    \n",
    "    # Conv層2\n",
    "    if params['n_conv_layers'] >= 2:\n",
    "        model.add(layers.Conv1D(\n",
    "            filters=params['filters_2'],\n",
    "            kernel_size=params['kernel_size_2'],\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(params['dropout_conv']))\n",
    "    \n",
    "    # Conv層3\n",
    "    if params['n_conv_layers'] >= 3:\n",
    "        model.add(layers.Conv1D(\n",
    "            filters=params['filters_3'],\n",
    "            kernel_size=params['kernel_size_3'],\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(params['dropout_conv']))\n",
    "    \n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    \n",
    "    # Dense層1\n",
    "    model.add(layers.Dense(params['dense_units_1'], activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(params['dropout_dense']))\n",
    "    \n",
    "    # Dense層2\n",
    "    if params['n_dense_layers'] >= 2:\n",
    "        model.add(layers.Dense(params['dense_units_2'], activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(params['dropout_dense']))\n",
    "    \n",
    "    # Dense層3\n",
    "    if params['n_dense_layers'] >= 3:\n",
    "        model.add(layers.Dense(params['dense_units_3'], activation='relu'))\n",
    "        model.add(layers.Dropout(params['dropout_dense']))\n",
    "    \n",
    "    # 出力層\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # コンパイル\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=params['learning_rate']),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_with_stratified_kfold(X, y, X_test, best_params_cnn, best_params_randomforest, best_params_catboost, best_params_xgboost, best_params_lightgbm, n_folds=5):\n",
    "    \"\"\"\n",
    "    Stratified K-Foldで複数モデルを学習し、アンサンブル予測を作成\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # OOF予測とテスト予測を保存\n",
    "    oof_preds_cnn = np.zeros(len(X))\n",
    "    oof_preds_randomforest = np.zeros(len(X))\n",
    "    oof_preds_catboost = np.zeros(len(X))\n",
    "    oof_preds_xgboost = np.zeros(len(X))\n",
    "    oof_preds_lightgbm = np.zeros(len(X))\n",
    "    \n",
    "    test_preds_cnn = np.zeros((len(X_test), n_folds))\n",
    "    test_preds_randomforest = np.zeros((len(X_test), n_folds))\n",
    "    test_preds_catboost = np.zeros((len(X_test), n_folds))\n",
    "    test_preds_xgboost = np.zeros((len(X_test), n_folds))\n",
    "    test_preds_lightgbm = np.zeros((len(X_test), n_folds))\n",
    "    \n",
    "    models_cnn = []\n",
    "    models_randomforest = []\n",
    "    models_catboost = []\n",
    "    models_xgboost = []\n",
    "    models_lightgbm = []\n",
    "    scalers = []\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Stratified K-Fold Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        # CNN用データ正規化\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_valid_scaled = scaler.transform(X_valid)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "        X_valid_cnn = X_valid_scaled.reshape(X_valid_scaled.shape[0], X_valid_scaled.shape[1], 1)\n",
    "        X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "        \n",
    "        scalers.append(scaler)\n",
    "        \n",
    "        # CNN\n",
    "        print(\"\\n[1/5] Training CNN...\")\n",
    "        model_cnn = build_cnn_model(best_params_cnn, (X_train_cnn.shape[1], 1))\n",
    "        \n",
    "        early_stop = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        model_cnn.fit(\n",
    "            validation_data=(X_valid_cnn, y_valid),\n",
    "            epochs=best_params_cnn['epochs'],\n",
    "            batch_size=best_params_cnn['batch_size'],\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        oof_preds_cnn[valid_idx] = (model_cnn.predict(X_valid_cnn, verbose=0).flatten() >= 0.5).astype(int)\n",
    "        test_preds_cnn[:, fold] = (model_cnn.predict(X_test_cnn, verbose=0).flatten() >= 0.5).astype(int)\n",
    "        models_cnn.append(model_cnn)\n",
    "        \n",
    "        acc_cnn = accuracy_score(y_valid, oof_preds_cnn[valid_idx])\n",
    "        print(f\"  Validation Accuracy: {acc_cnn:.4f}\")\n",
    "        \n",
    "        # RandomForest\n",
    "        print(\"\\n[2/5] Training RandomForest...\")\n",
    "        params_rf = best_params_randomforest.copy()\n",
    "        params_rf.update({'random_state': RANDOM_STATE, 'n_jobs': -1})\n",
    "        \n",
    "        if not params_rf.get('bootstrap', True):\n",
    "            params_rf.pop('max_samples', None)\n",
    "        \n",
    "        model_rf = RandomForestClassifier(**params_rf)\n",
    "        model_rf.fit(X_train, y_train)\n",
    "        \n",
    "        oof_preds_randomforest[valid_idx] = model_rf.predict(X_valid)\n",
    "        test_preds_randomforest[:, fold] = model_rf.predict(X_test)\n",
    "        models_randomforest.append(model_rf)\n",
    "        \n",
    "        acc_rf = accuracy_score(y_valid, model_rf.predict(X_valid))\n",
    "        print(f\"  Validation Accuracy: {acc_rf:.4f}\")\n",
    "        \n",
    "        # CatBoost\n",
    "        print(\"\\n[3/5] Training CatBoost...\")\n",
    "        params_cat = best_params_catboost.copy()\n",
    "        params_cat.update({'random_state': RANDOM_STATE, 'verbose': 0})\n",
    "        \n",
    "        model_cat = CatBoostClassifier(**params_cat)\n",
    "        model_cat.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=50, verbose=0)\n",
    "        \n",
    "        oof_preds_catboost[valid_idx] = model_cat.predict(X_valid)\n",
    "        test_preds_catboost[:, fold] = model_cat.predict(X_test)\n",
    "        models_catboost.append(model_cat)\n",
    "        \n",
    "        acc_cat = accuracy_score(y_valid, model_cat.predict(X_valid))\n",
    "        print(f\"  Validation Accuracy: {acc_cat:.4f}\")\n",
    "        \n",
    "        # XGBoost\n",
    "        print(\"\\n[4/5] Training XGBoost...\")\n",
    "        params_xgb = best_params_xgboost.copy()\n",
    "        params_xgb.update({'random_state': RANDOM_STATE, 'eval_metric': 'logloss'})\n",
    "        \n",
    "        model_xgb = XGBClassifier(**params_xgb)\n",
    "        model_xgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=50, verbose=0)\n",
    "        \n",
    "        oof_preds_xgboost[valid_idx] = model_xgb.predict(X_valid)\n",
    "        test_preds_xgboost[:, fold] = model_xgb.predict(X_test)\n",
    "        models_xgboost.append(model_xgb)\n",
    "        \n",
    "        acc_xgb = accuracy_score(y_valid, model_xgb.predict(X_valid))\n",
    "        print(f\"  Validation Accuracy: {acc_xgb:.4f}\")\n",
    "        \n",
    "        # LightGBM\n",
    "        print(\"\\n[5/5] Training LightGBM...\")\n",
    "        params_lgb = best_params_lightgbm.copy()\n",
    "        params_lgb.update({'random_state': RANDOM_STATE, 'verbose': -1})\n",
    "        \n",
    "        model_lgb = LGBMClassifier(**params_lgb)\n",
    "        model_lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[lgbm.early_stopping(50), lgbm.log_evaluation(0)])\n",
    "        \n",
    "        oof_preds_lightgbm[valid_idx] = model_lgb.predict(X_valid)\n",
    "        test_preds_lightgbm[:, fold] = model_lgb.predict(X_test)\n",
    "        models_lightgbm.append(model_lgb)\n",
    "        \n",
    "        acc_lgb = accuracy_score(y_valid, model_lgb.predict(X_valid))\n",
    "        print(f\"  Validation Accuracy: {acc_lgb:.4f}\")\n",
    "    \n",
    "    # OOFスコアを計算\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Out-of-Fold Scores\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"CNN OOF Accuracy: {accuracy_score(y, oof_preds_cnn):.4f}\")\n",
    "    print(f\"RandomForest OOF Accuracy: {accuracy_score(y, oof_preds_randomforest):.4f}\")\n",
    "    print(f\"CatBoost OOF Accuracy: {accuracy_score(y, oof_preds_catboost):.4f}\")\n",
    "    print(f\"XGBoost OOF Accuracy: {accuracy_score(y, oof_preds_xgboost):.4f}\")\n",
    "    print(f\"LightGBM OOF Accuracy: {accuracy_score(y, oof_preds_lightgbm):.4f}\")\n",
    "    \n",
    "    # テスト予測を平均化\n",
    "    test_preds_cnn_avg = test_preds_cnn.mean(axis=1)\n",
    "    test_preds_randomforest_avg = test_preds_randomforest.mean(axis=1)\n",
    "    test_preds_catboost_avg = test_preds_catboost.mean(axis=1)\n",
    "    test_preds_xgboost_avg = test_preds_xgboost.mean(axis=1)\n",
    "    test_preds_lightgbm_avg = test_preds_lightgbm.mean(axis=1)\n",
    "    \n",
    "    return {\n",
    "        'oof_cnn': oof_preds_cnn,\n",
    "        'oof_randomforest': oof_preds_randomforest,\n",
    "        'oof_catboost': oof_preds_catboost,\n",
    "        'oof_xgboost': oof_preds_xgboost,\n",
    "        'oof_lightgbm': oof_preds_lightgbm,\n",
    "        'test_cnn': test_preds_cnn_avg,\n",
    "        'test_randomforest': test_preds_randomforest_avg,\n",
    "        'test_catboost': test_preds_catboost_avg,\n",
    "        'test_xgboost': test_preds_xgboost_avg,\n",
    "        'test_lightgbm': test_preds_lightgbm_avg,\n",
    "        'models_cnn': models_cnn,\n",
    "        'models_randomforest': models_randomforest,\n",
    "        'models_catboost': models_catboost,\n",
    "        'models_xgboost': models_xgboost,\n",
    "        'models_lightgbm': models_lightgbm,\n",
    "        'scalers': scalers\n",
    "    }\n",
    "\n",
    "\n",
    "# LightGBMの最適化（まだ実行されていない場合）\n",
    "print(\"=\"*60)\n",
    "print(\"LightGBM Hyperparameter Optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study_lightgbm = optuna.create_study(direction='maximize', sampler=TPESampler(seed=RANDOM_STATE))\n",
    "study_lightgbm.optimize(lambda trial: objective_lightgbm(trial, X, y, N_FOLDS), \n",
    "                        n_trials=N_TRIALS, timeout=TIMEOUT, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest LightGBM Score: {study_lightgbm.best_value:.4f}\")\n",
    "print(f\"Best LightGBM Params: {study_lightgbm.best_params}\")\n",
    "\n",
    "best_params_lightgbm = study_lightgbm.best_params\n",
    "\n",
    "# K-Fold学習実行\n",
    "import lightgbm as lgbm\n",
    "\n",
    "results = train_with_stratified_kfold(\n",
    "    X, y, X_test,\n",
    "    best_params_cnn,\n",
    "    best_params_randomforest,\n",
    "    best_params_catboost,\n",
    "    best_params_xgboost,\n",
    "    best_params_lightgbm,\n",
    "    n_folds=N_FOLDS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. アンサンブル予測と提出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アンサンブル予測（5モデルの多数決）\n",
    "test_preds_ensemble = (\n",
    "    (results['test_cnn'] >= 0.5).astype(int) +\n",
    "    (results['test_randomforest'] >= 0.5).astype(int) +\n",
    "    (results['test_catboost'] >= 0.5).astype(int) +\n",
    "    (results['test_xgboost'] >= 0.5).astype(int) +\n",
    "    (results['test_lightgbm'] >= 0.5).astype(int)\n",
    ") / 5\n",
    "\n",
    "# 0.5を閾値として予測\n",
    "final_predictions = (test_preds_ensemble >= 0.5).astype(int)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Ensemble Predictions (5 Models: CNN + Tree-based)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Survived (0): {(final_predictions == 0).sum()} ({(final_predictions == 0).sum() / len(final_predictions) * 100:.1f}%)\")\n",
    "print(f\"Perished (1): {(final_predictions == 1).sum()} ({(final_predictions == 1).sum() / len(final_predictions) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出ファイルの作成\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Perished': final_predictions\n",
    "})\n",
    "\n",
    "# 出力ディレクトリの作成\n",
    "os.makedirs('../output', exist_ok=True)\n",
    "output_path = '../output/submission_optuna_stratified.csv'\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n提出ファイルを保存: {output_path}\")\n",
    "print(f\"\\n最初の10行:\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルと結果を保存\n",
    "model_save_path = '../output/models_optuna_stratified.pkl'\n",
    "\n",
    "# CNNモデルは別途保存（Kerasモデル）\n",
    "for i, cnn_model in enumerate(results['models_cnn']):\n",
    "    cnn_model.save(f'../output/cnn_model_fold_{i+1}.h5')\n",
    "\n",
    "# その他のモデルとパラメータを保存\n",
    "with open(model_save_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'models_randomforest': results['models_randomforest'],\n",
    "        'models_catboost': results['models_catboost'],\n",
    "        'models_xgboost': results['models_xgboost'],\n",
    "        'models_lightgbm': results['models_lightgbm'],\n",
    "        'scalers': results['scalers'],\n",
    "        'best_params_cnn': best_params_cnn,\n",
    "        'best_params_randomforest': best_params_randomforest,\n",
    "        'best_params_catboost': best_params_catboost,\n",
    "        'best_params_xgboost': best_params_xgboost,\n",
    "        'best_params_lightgbm': best_params_lightgbm\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\nモデルを保存: {model_save_path}\")\n",
    "print(f\"CNNモデルを保存: ../output/cnn_model_fold_*.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"すべての処理が完了しました!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n最終結果:\")\n",
    "print(f\"  CNN OOF Score: {accuracy_score(y, results['oof_cnn']):.4f}\")\n",
    "print(f\"  RandomForest OOF Score: {accuracy_score(y, results['oof_randomforest']):.4f}\")\n",
    "print(f\"  CatBoost OOF Score: {accuracy_score(y, results['oof_catboost']):.4f}\")\n",
    "print(f\"  XGBoost OOF Score: {accuracy_score(y, results['oof_xgboost']):.4f}\")\n",
    "print(f\"  LightGBM OOF Score: {accuracy_score(y, results['oof_lightgbm']):.4f}\")\n",
    "print(f\"\\n提出ファイル: {output_path}\")\n",
    "print(f\"モデルファイル: {model_save_path}\")\n",
    "print(f\"CNNモデル: ../output/cnn_model_fold_*.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
